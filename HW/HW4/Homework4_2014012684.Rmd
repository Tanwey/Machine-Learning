---
title: "Machine Learning Homework 4"
author: "He Li, 2014012684"
date: "2017/10/31"
output: pdf_document
---

\newtheorem*{pb}{Problem}

\begin{pb}
Given a matrix $A = (a_{ij})_{n \times m}$, show that $\min_{i \leq n}\max_{j \leq m} a_{ij} \geq \max_{i \leq m} \min_{j \leq n} a_{ji}$.
\end{pb}

\begin{proof}
Denote $\min_{i \leq n}\max_{j \leq m} a_{ij} = a_{pq}$, $\max_{i \leq m} \min_{j \leq n} a_{ji} = a_{rt}$. Then
$$
a_{pq} \geq a_{rq} \geq a_{rt}
$$
Therefore, 
$$
\min_{i \leq n}\max_{j \leq m} a_{ij} \geq \max_{i \leq m} \min_{j \leq n} a_{ji}
$$
\end{proof}

\begin{pb}
Show that KKT conditions are necessary and if $f, g_i$ are convex and each $h_i$ is linear then itâ€™s also sufficie for $(X^*, \lambda^*, \mu^*)$ to be the optima of primal and dual programmings.
\end{pb}

\begin{proof}
First we show that KKT conditions are necessary. Denote $(x^*, \lambda^*,\mu^*)$ the solution of primal and dual problems. Obviously, it satisfies (1), (2) and (3) of KKT conditions. For (4), consider the primal problem, if $g_i(x^*) < 0$, then $\lambda_i(x^*)$ should equal to 0 in order to max the primal function. Therefore, $\lambda_i g_i(x^*) = 0$.

Then we show that with the additional condition that $f(x), g_i(x)$ are convex and $h_i(x)$ is linear, KKT condition is sufficient. 

First, a $x_0$ satisties the above conditions also satisfies the constraints in primal and dual problems. Denote $x1$ the solution of primal problem and $x_2$ the solution of dual problem, deonte $L(x,\mu,\lambda) = f(x) + \sum\limits_i \mu_i h_i(x) + \sum\limits_i \lambda_i g_i(x)$. 

With the additional condition, $L(x,\mu,\lambda)$ is a convex function w.r.t $x$, then $x_0 = \arg\min\limits_x L(x,\mu,\lambda)$. Therefore, 
\begin{align}
\label{eqn1}
L(x_0,\mu,\lambda) \leq L(x_2,\mu,\lambda) \leq L(x_1,\mu,\lambda)
\end{align}
Note that $x_0$ satisfies primal constraints and $x_2$ is the argmin of primal problem. Therefore, 
\begin{align}
\label{eqn2}
L(x_1,\mu,\lambda) \leq L(x_0,\mu,\lambda)
\end{align}
With (\ref{eqn1}) and (\ref{eqn2}) we have our conclusion that
$$
L(x_0,\mu,\lambda) = L(x_2,\mu,\lambda) = L(x_1,\mu,\lambda)
$$
\end{proof}






